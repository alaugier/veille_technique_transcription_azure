@article{AttentionIsAllYouNeed,
  title={{Attention Is All You Need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}

@inproceedings{Informer,
  title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={7},
  pages={7584--7592},
  year={2021},
  eprint={2012.07436},
  url={https://arxiv.org/abs/2012.07436}
}

@article{Autoformer,
  title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22433--22445},
  year={2021},
  eprint={2106.13008},
  url={https://arxiv.org/abs/2106.13008}
}

@article{DLinear,
  title={Are Transformers Effective for Time Series Forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={7},
  pages={8736--8744},
  year={2023},
  eprint={2205.13504},
  url={https://arxiv.org/abs/2205.13504}
}

@article{PatchTST,
  title={A Time Series is Worth 64 Words: Long-Term Forecasting with Transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023},
  eprint={2211.14730},
  url={https://arxiv.org/abs/2211.14730}
}

@inproceedings{iTransformer,
  title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
  author={Liu, Yong and Hu, Tengge et al.},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  eprint={2310.06625},
  url={https://arxiv.org/abs/2310.06625}
}

@inproceedings{NBEATS,
  title={N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting},
  author={Oreshkin, Boris N et al.},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  eprint={1905.10437},
  url={https://arxiv.org/abs/1905.10437}
}

@article{TiDE,
  title={Long-term Forecasting with TIDE: Time-series Dense Encoder},
  author={Das, Abhimanyu et al.},
  journal={arXiv preprint arXiv:2304.08424},
  year={2024},
  eprint={2304.08424},
  url={https://arxiv.org/abs/2304.08424}
}

@article{TSMixer,
  title={TSMixer: An All-MLP Architecture for Time Series Forecasting},
  author={Chen, Si-An et al.},
  journal={Transactions on Machine Learning Research},
  year={2023},
  eprint={2303.06053},
  url={https://arxiv.org/abs/2303.06053}
}

@inproceedings{TimesNet,
  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  author={Wu, Haixu et al.},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  eprint={2210.02186},
  url={https://arxiv.org/abs/2210.02186}
}

@article{DishTS,
  title={Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting},
  author={Fan, Wei et al.},
  journal={arXiv preprint arXiv:2302.14829},
  year={2023},
  eprint={2302.14829},
  url={https://arxiv.org/abs/2302.14829}
}

@article{TimeGPT,
  title={TimeGPT-1},
  author={Garza, Azul et al.},
  journal={arXiv preprint arXiv:2310.03589},
  year={2024},
  eprint={2310.03589},
  url={https://arxiv.org/abs/2310.03589}
}

@article{LLMTime,
  title={Large Language Models Are Zero-Shot Time Series Forecasters},
  author={Gruver, Nate et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  eprint={2310.07820},
  url={https://arxiv.org/abs/2310.07820}
}

@article{Chronos,
  title={Chronos: Learning the Language of Time Series},
  author={Ansari, Abdul Fatir et al.},
  journal={Transactions on Machine Learning Research},
  year={2024},
  eprint={2403.07815},
  url={https://arxiv.org/abs/2403.07815}
}

@inproceedings{TimeLLM,
  title={TIME-LLM: Time Series Forecasting by Reprogramming Large Language Models},
  author={Jin, Ming et al.},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  eprint={2310.01728},
  url={https://arxiv.org/abs/2310.01728}
}

@article{TFT,
  title={Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
  author={Lim, Bryan et al.},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1743--1754},
  year={2021},
  url={https://arxiv.org/abs/1912.09363}
}


@online{AzureSpeechDoc,
  author    = {Microsoft},
  title     = {Azure Speech Service Documentation},
  year      = {2024},
  url       = {https://learn.microsoft.com/fr-fr/azure/ai-services/speech-service/},
  note      = {Consulté en juin 2024}
}

@online{GoogleSpeechDoc,
  author    = {Google},
  title     = {Cloud Speech-to-Text Documentation},
  year      = {2024},
  url       = {https://cloud.google.com/speech-to-text/docs},
  note      = {Consulté en juin 2024}
}

@online{AWS_Transcribe,
  author    = {Amazon},
  title     = {AWS Transcribe Documentation},
  year      = {2024},
  url       = {https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html},
  note      = {Consulté en juin 2024}
}

@online{WhisperOpenAI,
  author    = {OpenAI},
  title     = {Whisper: Robust Speech Recognition},
  year      = {2022},
  url       = {https://openai.com/research/whisper},
  note      = {Consulté en juin 2024}
}

@online{BenchmarkSpeech,
  author    = {HuggingFace},
  title     = {Whisper Benchmarks},
  year      = {2024},
  url       = {https://huggingface.co/openai/whisper-large-v3},
  note      = {Consulté en juin 2024}
}

@article{radford2022whisper,
  title={Robust Speech Recognition via Large-Scale Weak Supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and others},
  journal={arXiv preprint arXiv:2212.04356},
  year={2022},
  url={https://arxiv.org/abs/2212.04356}
}

@online{pwcspeech,
  title={Speech Recognition Benchmarks},
  author={Papers With Code},
  year={2024},
  url={https://paperswithcode.com/task/speech-recognition},
  note={Consulté en juin 2024}
}

@online{azure2024speech,
  title={Azure Cognitive Services Speech Documentation},
  author={Microsoft},
  year={2024},
  url={https://learn.microsoft.com/en-us/azure/ai-services/speech-service/}
}

@online{google2024speech,
  title={Google Cloud Speech-to-Text Documentation},
  author={Google Cloud},
  year={2024},
  url={https://cloud.google.com/speech-to-text/docs}
}

@online{assemblyai2023,
  title={The Top Free Speech-to-Text APIs and Open Source Engines},
  author={AssemblyAI},
  year={2023},
  url={https://www.assemblyai.com/blog/the-top-free-speech-to-text-apis-and-open-source-engines/},
  note={Consulté en juin 2024}
}

@online{deepgram2023,
  title={Speech-to-Text Accuracy Comparison},
  author={Deepgram},
  year={2023},
  url={https://deepgram.com/learn/speech-to-text-accuracy-comparison},
  note={Consulté en juin 2024}
}

